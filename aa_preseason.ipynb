{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85668b9-7a9c-4791-af74-1ef6a5e773dc",
   "metadata": {},
   "source": [
    "This script is the main flood AA script that we run prior to the rainy season beginning in October. This script evaluates the performance of the Global Flood Awareness System (GloFAS) against observation based on past forecasts. We use this information to choose statistically backed trigger for anticipatory action. The goal is to understand what percentage of ensemble members from the forecast are needed to implement action on the ground."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d463e2-7054-44b4-83f6-236737c7634b",
   "metadata": {},
   "source": [
    "As usual we start by importing all relevant packages saved in our aa-env. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554ae17-87b5-41a0-a32c-89fb7288044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d5f69-a42c-4e16-ab2c-b6382293ffb1",
   "metadata": {},
   "source": [
    "Section 1: define variables, paths and read in data\n",
    "\n",
    "In this first section we will simply define the relevant paths to where our forecast, observed and metadata are stored and load the data into Python. Remember to download this data from the README file if you haven't already done so. \n",
    "\n",
    "Ensure that your main directory and country of choice are correct. You should already have all the folders set-up. The script will automatically create the output folder in which we will store any results produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cef13-6c80-4269-bb6f-317732adb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer to track progress\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d85b1-5980-4a44-8ae7-6c998b9d90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'zimbabwe'  # Define country of interest\n",
    "directory = '/Users/jamietowner/Documents/flood_aa_training/'  # Define directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75004172-0c78-44b4-84bf-9a4369451c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to data\n",
    "forecast_data_directory = os.path.join(directory, country, \"data/forecasts\")\n",
    "observed_data_directory = os.path.join(directory, country, \"data/reanalysis\")\n",
    "metadata_directory = os.path.join(directory, country, \"data/metadata\")\n",
    "output_directory = os.path.join(directory, country, \"outputs\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535af61-fd8b-4e35-98f5-d009ae2f3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filenames\n",
    "observed_data_file = \"glofas_observations.csv\"\n",
    "station_info_file = \"metadata_zim.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfed37-0eeb-4e65-bea2-0b5d9840b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the observed data and gauging stations metadata\n",
    "observed_data_path = os.path.join(observed_data_directory, observed_data_file)\n",
    "station_info_path = os.path.join(metadata_directory, station_info_file)\n",
    "observed_data = pd.read_csv(observed_data_path)\n",
    "station_info = pd.read_csv(station_info_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196206f-16a2-4801-a9dd-bbdf9b1fde67",
   "metadata": {},
   "source": [
    "The next two cells we can quickly check the data input is what we expected. We can use different ways to do this such as df.columns or df to visualise the column headers or see the entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c472e-0669-4241-a917-c0e51e45c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show column headers of dataframe \n",
    "observed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c7385-0c75-45f2-835d-1dcf324af398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataframe \n",
    "station_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce773c4f-f168-4238-8ab6-6248ea8212a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column in observed_data to pandas timestamps \n",
    "observed_data[\"date\"] = pd.to_datetime(observed_data[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd350e-0e3d-42b5-9cc7-d2bf442fc228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all GloFAS forecast files\n",
    "forecast_files = glob.glob(os.path.join(forecast_data_directory, '*.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af381100-68ff-4cfd-9ad0-d4d8e79016c4",
   "metadata": {},
   "source": [
    "In the below cell, we use what is know as a print command to tell Python we would like it to print the output of a variable. In this case the relevant data directories. Printing variables and paths is a good way to check everything is correct and a good way to debug simple errors in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0fbe3-8696-44dd-9a85-f0880b8e4f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print paths to ensure they are set correctly\n",
    "print(f\"\"\"\n",
    "forecast directory: {forecast_data_directory}\n",
    "observed data directory: {observed_data_directory}\n",
    "metadata directory: {metadata_directory}\n",
    "output directory: {output_directory}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86aee45-1ca9-412a-ba13-a5d13220c46c",
   "metadata": {},
   "source": [
    "Section 2: process observations and forecasts and define events/non-events\n",
    "\n",
    "In this section we will be processing the observation and forecasts by looping over the different dimensions of the forecast data, such as the lead times (out to 46 days), ensemble members (11 for the hindcast data) and each of the forecast issue dates between 2003 and 2023. The script takes each forecast and defines the forecast issue from this information. It then finds the forecast end date (for each lead time) and extract the observed and forecasts value for this end date. It then evaluates whether or not this value (for both observed and forecasted data) exceeds the defined thresholds. If it does it assigns a 1 (or TRUE) and if it does not it assigns a 0 (or FALSE). \n",
    "\n",
    "The script finishes by creating a dictionary of all the forecast and stores information such as the forecast issue date, forecast end date, the threshold category, the ensemble number and if there was an observed or forecasted flood event or not (i.e., based on the 0's and 1's). Finally, we create a dataframe so we can manipulate this data to calculate our triggers further down in the script. \n",
    "\n",
    "For this training we will be processing three gauging stations and two thresholds (moderate and severe) as it can take quite a long time to run for all stations (up to a day!). We will be running this script for all stations on a server nearer to the time of the 2024/25 rainy season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3a2b2-8f2b-4413-bc2c-e1f430791a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store events/non-events \n",
    "events = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418220c-cc4c-453d-a0b4-ac92a5ccbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each forecast file \n",
    "for forecast_file in tqdm(forecast_files, desc=\"Processing forecast files\"):\n",
    "    # Load the NetCDF file\n",
    "    ds = xr.open_dataset(forecast_file)\n",
    "    \n",
    "    # Extract all ensemble members from the 'number' dimension\n",
    "    ensemble_members = ds['dis24'].coords['number'].values  # Assumes 'number' is the dimension for ensemble members\n",
    "\n",
    "\n",
    "    # Extract the forecast issue date from the file and convert to pandas datetime\n",
    "    forecast_issue_ns = ds['time'].values.item()  # Get the single scalar value (nanoseconds since epoch)\n",
    "    forecast_issue_date = pd.to_datetime(forecast_issue_ns, unit='ns')\n",
    "\n",
    "    # Define the lead times\n",
    "    lead_times = list(range(0, 15))  # 0 up to 46 days\n",
    "\n",
    "    # Loop over station metadata to extract information for each station\n",
    "    for index, station_row in station_info.iterrows():\n",
    "        station_name = station_row[\"station name\"]\n",
    "        station_lat = station_row[\"lat\"]\n",
    "        station_lon = station_row[\"lon\"]\n",
    "\n",
    "        # Extract individual station thresholds from metadata file\n",
    "        thresholds = {\n",
    "           # \"bankfull\": (station_row[\"obs_bankfull\"], station_row[\"glofas_bankfull\"]),\n",
    "            \"moderate\": (station_row[\"obs_moderate\"], station_row[\"glofas_moderate\"]),\n",
    "            \"severe\": (station_row[\"obs_severe\"], station_row[\"glofas_severe\"]),\n",
    "        }\n",
    "        \n",
    "        # Process each lead time\n",
    "        for lead_time in lead_times:\n",
    "            # Calculate the forecast end date based on lead time\n",
    "            forecast_end_date = forecast_issue_date + pd.DateOffset(days=lead_time)\n",
    "            \n",
    "            # Filter observed data for the matching period\n",
    "            observed_period = observed_data[observed_data[\"date\"] == forecast_end_date]\n",
    "            \n",
    "            # Skip if no observation data is available for this period\n",
    "            if observed_period.empty:\n",
    "                continue\n",
    "\n",
    "            observed_values = observed_period[station_name].values[0]\n",
    "\n",
    "            # Skip if there's no observation data (NaN value) for the specific station\n",
    "            if pd.isnull(observed_values):\n",
    "                continue\n",
    "\n",
    "            # Extract forecast values for each ensemble member\n",
    "            for ensemble_member in ensemble_members:\n",
    "                forecast_data = ds['dis24'].sel(number=ensemble_member).isel(step=lead_time).values\n",
    "\n",
    "                # Loop over the thresholds\n",
    "                for severity, (obs_threshold, sim_threshold) in thresholds.items():\n",
    "                    # Define events and non-events\n",
    "                    observed_event = observed_values > obs_threshold\n",
    "                    forecast_event = forecast_data > sim_threshold\n",
    "                    \n",
    "                    # Define tolerant events and non-events (within ±2 days window)\n",
    "                    observed_window = observed_data[(observed_data[\"date\"] >= forecast_end_date - pd.DateOffset(days=2)) & \n",
    "                                                    (observed_data[\"date\"] <= forecast_end_date + pd.DateOffset(days=2))]\n",
    "                    tolerant_observed_event = (observed_window[station_name] > obs_threshold).any()\n",
    "                    \n",
    "                    # Create a dictionary to store results\n",
    "                    events_dict = {\n",
    "                        \"forecast file\": os.path.basename(forecast_file),\n",
    "                        \"lead time\": lead_time,\n",
    "                        \"station name\": station_name,\n",
    "                        \"ensemble member\": ensemble_member,\n",
    "                        \"forecasted date\": forecast_end_date.date(),\n",
    "                        \"threshold\": severity,\n",
    "                        \"observed event\": observed_event,\n",
    "                        \"forecast event\": forecast_event,\n",
    "                        \"tolerant observed event\": tolerant_observed_event\n",
    "                    }\n",
    "            \n",
    "                    # Append the events dictionary to the list\n",
    "                    events.append(events_dict)\n",
    "\n",
    "# Create a data frame from the list of event dictionaries\n",
    "events_df = pd.DataFrame(events)\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc947a1-6d49-4793-a8ab-322748fe37ee",
   "metadata": {},
   "source": [
    "Section 3: construct contigency table and skill score metrics\n",
    "\n",
    "In this section we will be using the dataframe produced above to construct contigency table and verification metrics to assess the skill of our forecasts at each station for different lead times. \n",
    "\n",
    "The contigency table is based on the hits, misses, false alarms and correct rejections which allows us to calculate metrics such as hit and false alarm rates. This tells us based on past forecasts how well the system would have performed in the past. If the forecast is very good then the hit rate will be closer to 1 and if it is particularly bad it will be closer to 0. We aim to have a hit rate above 0.5 for each station. \n",
    "\n",
    "As we are using ensemble data we have 11 predicted values at each lead time. When analysing the table we have TRUE or FALSE (or 1's and 0's) for the observed event and then 11 values of TRUE or FALSE. To make it easier to analyse we first calculate what is called the probability of detection. The is simply summing the number of 1's (i.e., hits) in a single forecasts from the 11 members and then diving by the total number of ensemble members (11 in our case). This provides us with a percentage which show how likely or unlikely the forecast thought a potential flood could be. This is key when analysing and choosing triggers further down the script.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3585b94-0f91-49d3-94fd-3bfbd06b4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot the events_df to list ensemble members as columns\n",
    "pivot_events_df = events_df.pivot_table(index=[\"forecast file\", \"lead time\", \"station name\", \"forecasted date\",\"threshold\", \"observed event\", \"tolerant observed event\"],\n",
    "                                        columns=\"ensemble member\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61baa46-b6d3-4116-b553-e418c41823a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index to convert the pivoted dataframe to a flat table structure\n",
    "pivot_events_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5499669-8f39-470f-afa9-9551af175953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the columns corresponding to the forecast ensemble members (5 to 16)\n",
    "ensemble_member_columns = pivot_events_df.columns[7:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77017e19-c4af-4051-acaa-30fc51451fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of ensemble members with events (i.e., 1's) for each row\n",
    "pivot_events_df[\"probability of detection\"] = pivot_events_df[ensemble_member_columns].sum(axis=1) / len(ensemble_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c2daa3-30bf-4d85-8c9a-6f378681fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trigger thresholds from 0.01 to 0.99 with a step size of 0.01\n",
    "thresholds = np.arange(0.01, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd07d6-6440-4aa9-a05c-481f1367b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty dataframe to store the event occurrence for each threshold\n",
    "metrics_df = pd.DataFrame(index=pivot_events_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447c4e9-19d4-485e-a768-9bc041c4c090",
   "metadata": {},
   "source": [
    "The next part of the script loops over the thresholds variable which we just defined from 0.01 to 1. It analyses each forecast to see if the probability of detection exceeds each triggers value. This is the same step that we performed yesterday in the excel session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb50692-bcdb-47c7-ac64-807fc94d0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each threshold\n",
    "for threshold in thresholds:\n",
    "    # determine if the forecast event occurs based on the threshold\n",
    "    event_occurrence = (pivot_events_df['probability of detection'] >= threshold).astype(int)\n",
    "    \n",
    "    # add the event occurrence as a column in the event_df DataFrame\n",
    "    metrics_df[f'threshold_{threshold:.2f}'] = event_occurrence\n",
    "\n",
    "# concatenate the forecast file and observed event columns from result_df to event_df\n",
    "metrics_df = pd.concat([pivot_events_df[['forecast file','station name', 'lead time','forecasted date','threshold','observed event','tolerant observed event','probability of detection']], metrics_df], axis=1)\n",
    "\n",
    "# flatten the multi-index columns\n",
    "metrics_df.columns = [''.join(col).strip() for col in metrics_df.columns.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50332184-ecbe-44d6-8f66-b675e5eb4188",
   "metadata": {},
   "source": [
    "In the next cell we define different ranges of lead times (e.g., 10-15 days) in order to evaluate the performance of the forecast at different timescales and see how far we can skillfully predict into the future. We then group our huge dataframe of all stations, lead times, thresholds and filter it into lots of smaller dataframes based on the station name, lead time category and threshold. We can then evaulate each station easier and in more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8194a3a-2793-4a61-867a-317ff6233ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the lead time ranges (optional, change as desired)\n",
    "bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 46, float('inf')]\n",
    "labels = ['0-5', '6-10', '11-15', '16-20', '21-25', '26-30', '31-35', '36-40', '41-46', '0-46']\n",
    "\n",
    "# create a new column called lead time category' that categorizes lead times into these ranges\n",
    "metrics_df['lead time category'] = pd.cut(metrics_df['lead time'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# group by station name, lead time category, and threshold\n",
    "grouped = metrics_df.groupby(['station name', 'lead time category', 'threshold'], observed=False)\n",
    "\n",
    "# create a dictionary to store each group's dataframe\n",
    "grouped_dfs = {name: group for name, group in grouped}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb9e18-405d-4466-9ce1-bb21db2227e6",
   "metadata": {},
   "source": [
    "The next cell create a function called 'calculate_metrics' and this is used to evaluate each of the individually filtered grouped dataframes which we created above. In this function we will calculate various metrics such as the number of hits, false alarms, misses, correct rejection, hit and false alarm rates and some metrics such as the critical success index (CSI). The CSI tells us how well the forecasting system balances the hit rate and false alarm ratio and is a key indicator for anticipatory action systems for determining triggers (i.e., what percentage of ensemble members will we be willing to implement anticipatory action on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53787396-0c46-4ccb-9535-25e1d11db478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate verification metrics metrics for each grouped dataframe \n",
    "def calculate_metrics(df):\n",
    "    hits = {}\n",
    "    false_alarms = {}\n",
    "    misses = {}\n",
    "    correct_rejections = {}\n",
    "    hit_rate = {}\n",
    "    false_alarm_rate = {}\n",
    "    csi = {}\n",
    "    pss = {}\n",
    "    f1_scores = {}\n",
    "    \n",
    "\n",
    "    # calculate contigency table metrics for each trigger threshold column\n",
    "    for column in df.columns[8:107]:  # check columns from index 8 onwards are thresholds\n",
    "        hits[column] = ((df['observed event'] == 1) & (df[column] == 1)).sum()\n",
    "        false_alarms[column] = ((df['observed event'] == 0) & (df[column] == 1)).sum()\n",
    "        misses[column] = ((df['observed event'] == 1) & (df[column] == 0)).sum()\n",
    "        correct_rejections[column] = ((df['observed event'] == 0) & (df[column] == 0)).sum()\n",
    "       \n",
    "        # calculate verification metrics\n",
    "        total_observed_events = hits[column] + misses[column]\n",
    "        total_forecasted_events = hits[column] + false_alarms[column]\n",
    "        hit_rate[column] = hits[column] / total_observed_events if total_observed_events > 0 else 0\n",
    "        false_alarm_rate[column] = false_alarms[column] / total_forecasted_events if total_forecasted_events > 0 else 0\n",
    "        csi[column] = hits[column] / (hits[column] + false_alarms[column] + misses[column]) if (hits[column] + false_alarms[column] + misses[column]) > 0 else 0\n",
    "        pss[column] = (hits[column] * correct_rejections[column] - false_alarms[column] * misses[column]) / \\\n",
    "                      ((hits[column] + misses[column]) * (false_alarms[column] + correct_rejections[column])) if (hits[column] + misses[column]) * (false_alarms[column] + correct_rejections[column]) > 0 else 0\n",
    "        precision = hits[column] / total_forecasted_events if total_forecasted_events > 0 else 0\n",
    "        recall = hit_rate[column]\n",
    "        f1_scores[column] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # convert the metrics dictionaries to dataframes\n",
    "    hits_df = pd.DataFrame(hits, index=['hits'])\n",
    "    false_alarms_df = pd.DataFrame(false_alarms, index=['false alarms'])\n",
    "    misses_df = pd.DataFrame(misses, index=['misses'])\n",
    "    correct_rejections_df = pd.DataFrame(correct_rejections, index=['correct rejections'])\n",
    "    hit_rate_df = pd.DataFrame(hit_rate, index=['hit rate'])\n",
    "    false_alarm_rate_df = pd.DataFrame(false_alarm_rate, index=['false alarm rate'])\n",
    "    \n",
    "    csi_df = pd.DataFrame(csi, index=['csi'])\n",
    "    pss_df = pd.DataFrame(pss, index=['pss'])\n",
    "    f1_scores_df = pd.DataFrame(f1_scores, index=['f1 score'])\n",
    " \n",
    "    # concatenate the metrics dataframes\n",
    "    metrics_df = pd.concat([\n",
    "        hits_df, false_alarms_df,\n",
    "        misses_df, correct_rejections_df, hit_rate_df, false_alarm_rate_df,\n",
    "        csi_df, pss_df, f1_scores_df,\n",
    "    ])\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409fc23-ed97-4e63-88e4-afdcfea858be",
   "metadata": {},
   "source": [
    "The cell below simply runs the function above over the group of dataframes that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6c6d7-9e6b-464e-8d59-c2a0d6bbf9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each dataframe in grouped_dfs, apply calculate_metrics, and store the results back into grouped_dfs\n",
    "for key, df in grouped_dfs.items():\n",
    "    grouped_dfs[key] = calculate_metrics(df)\n",
    "    \n",
    "    # timer end \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"elapsed time for calculating metrics: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a4198-33de-48c7-ad10-77fe18d87a18",
   "metadata": {},
   "source": [
    "To view the results of a specifc dataframe (i.e., the performance of one of the stations) run the cell below. Feel free to change the key inputs to one of the other stations, lead time categories or thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00acfe9-2384-4367-b9f8-6dc27dfb7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view a specific grouped dataframe \n",
    "specific_df = grouped_dfs[\"Nyakapupu\",'0-5','moderate']\n",
    "specific_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ff775-8dd6-4d2d-8b54-37518329c54c",
   "metadata": {},
   "source": [
    "Section 4: choosing triggers\n",
    "\n",
    "In this section we will be choosing triggers based on the results from each specific dataframes above. The code loops through each dataframe and finds the percentage trigger where the CSI metric is the highest. In the case of multiple triggers sharing the highest CSI it then looks to choose the metric with the highest hit rate. You can visualise the output of the dataframe in your output directory as a csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebda93-cd97-4a2c-80a0-42341078eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the best triggers\n",
    "best_triggers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e1579-939f-4d4e-8956-e192ebc43c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each dataframe in grouped_dfs\n",
    "for key, df in grouped_dfs.items():\n",
    "    # Find the column names corresponding to thresholds\n",
    "    threshold_columns = [col for col in df.columns if col.startswith('threshold_')]\n",
    "    \n",
    "    # Determine the maximum CSI value for each trigger\n",
    "    max_csi = df.loc['csi'].max()\n",
    "    \n",
    "    # Find all trigger with the maximum CSI value\n",
    "    best_csi_trigger = df.loc['csi'][df.loc['csi'] == max_csi].index.tolist()\n",
    "    \n",
    "    # If there are multiple triggers with the same CSI, choose the one with the highest hit rate\n",
    "    if len(best_csi_trigger) > 1:\n",
    "        # Find the hit rate for each of the best CSI triggers\n",
    "        hit_rates = df.loc['hit rate'][best_csi_trigger]\n",
    "        max_hit_rate = hit_rates.max()\n",
    "        \n",
    "        # Find all triggers with the maximum hit rate\n",
    "        best_hit_rate_trigger = hit_rates[hit_rates == max_hit_rate].index.tolist()\n",
    "        \n",
    "        # If there are still ties, choose the trigger with the lowest value\n",
    "        if len(best_hit_rate_trigger) > 1:\n",
    "            # Convert trigger names to numbers (assuming trigger names follow a specific format)\n",
    "            numeric_trigger = [float(thresh.split('_')[1]) for thresh in best_hit_rate_trigger]\n",
    "            best_trigger = best_hit_rate_trigger[np.argmin(numeric_trigger)]  # Get the lowest threshold\n",
    "        else:\n",
    "            best_trigger = best_hit_rate_trigger[0]\n",
    "    else:\n",
    "        best_trigger = best_csi_trigger[0]\n",
    "    \n",
    "    # Store the best trigger information\n",
    "    best_triggers[key] = {\n",
    "        'best_trigger': best_trigger,\n",
    "        'csi_value': df.loc['csi', best_trigger],\n",
    "        'hit_rate': df.loc['hit rate', best_trigger]\n",
    "    }\n",
    "\n",
    "# Convert the best_triggers dictionary to a DataFrame for better readability\n",
    "best_triggers_df = pd.DataFrame(best_triggers).T\n",
    "\n",
    "# Print the DataFrame with the best triggers\n",
    "print(best_triggers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb52aa-c9d5-4234-b6f7-8f8e25a939e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the best_triggers dictionary to a DataFrame for better readability\n",
    "best_triggers_df = pd.DataFrame(best_triggers).T\n",
    "best_triggers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891de48a-1a40-41f2-830f-751949eb6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "best_triggers_df.to_csv(os.path.join(output_directory, f'triggers_{country}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0dbfd-fd4d-4aa0-8015-d08342fb49d6",
   "metadata": {},
   "source": [
    "Congratulations! you have now completed the preseason script for the flood AA system. As mentioned we will be running this for all stations and thresholds prior to the rainy season in October 2025. We are now currently working on the operational script which will be used to monitor the daily forecasts from GloFAS to see if the triggers have been exceeded. This information will be updated in an online dashboard using PRISM similar to the drought AA set-up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d6093-4de8-4176-9144-c00de56ec6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

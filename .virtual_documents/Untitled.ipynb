





from tqdm import tqdm
import xarray as xr
import pandas as pd
import os
import glob
import numpy as np
import time





# Start timer to track progress
start_time = time.time()


country = 'zimbabwe'  # Define country of interest
directory = '/Users/jamietowner/Documents/flood_aa_training/'  # Define directory


# Define paths to data
forecast_data_directory = os.path.join(directory, country, "data/forecasts")
observed_data_directory = os.path.join(directory, country, "data/reanalysis")
metadata_directory = os.path.join(directory, country, "data/metadata")
output_directory = os.path.join(directory, country, "outputs")

# Create directories if they don't exist
os.makedirs(output_directory, exist_ok=True)


# Define filenames
observed_data_file = "glofas_observations.csv"
station_info_file = "metadata_zim.csv"


# Load the observed data and gauging stations metadata
observed_data_path = os.path.join(observed_data_directory, observed_data_file)
station_info_path = os.path.join(metadata_directory, station_info_file)
observed_data = pd.read_csv(observed_data_path)
station_info = pd.read_csv(station_info_path) 





# Show column headers of dataframe 
observed_data.columns


# Show dataframe 
station_info


# Convert the date column in observed_data to pandas timestamps 
observed_data["date"] = pd.to_datetime(observed_data["date"])


# Load all GloFAS forecast files
forecast_files = glob.glob(os.path.join(forecast_data_directory, '*.nc'))





# Print paths to ensure they are set correctly
print(f"""
forecast directory: {forecast_data_directory}
observed data directory: {observed_data_directory}
metadata directory: {metadata_directory}
output directory: {output_directory}
""")





# Create an empty list to store events/non-events 
events = []


# Loop over each forecast file 
for forecast_file in tqdm(forecast_files, desc="Processing forecast files"):
    # Load the NetCDF file
    ds = xr.open_dataset(forecast_file)
    
    # Extract all ensemble members from the 'number' dimension
    ensemble_members = ds['dis24'].coords['number'].values  # Assumes 'number' is the dimension for ensemble members


    # Extract the forecast issue date from the file and convert to pandas datetime
    forecast_issue_ns = ds['time'].values.item()  # Get the single scalar value (nanoseconds since epoch)
    forecast_issue_date = pd.to_datetime(forecast_issue_ns, unit='ns')

    # Define the lead times
    lead_times = list(range(0, 15))  # 0 up to 46 days

    # Loop over station metadata to extract information for each station
    for index, station_row in station_info.iterrows():
        station_name = station_row["station name"]
        station_lat = station_row["lat"]
        station_lon = station_row["lon"]

        # Extract individual station thresholds from metadata file
        thresholds = {
           # "bankfull": (station_row["obs_bankfull"], station_row["glofas_bankfull"]),
            "moderate": (station_row["obs_moderate"], station_row["glofas_moderate"]),
            "severe": (station_row["obs_severe"], station_row["glofas_severe"]),
        }
        
        # Process each lead time
        for lead_time in lead_times:
            # Calculate the forecast end date based on lead time
            forecast_end_date = forecast_issue_date + pd.DateOffset(days=lead_time)
            
            # Filter observed data for the matching period
            observed_period = observed_data[observed_data["date"] == forecast_end_date]
            
            # Skip if no observation data is available for this period
            if observed_period.empty:
                continue

            observed_values = observed_period[station_name].values[0]

            # Skip if there's no observation data (NaN value) for the specific station
            if pd.isnull(observed_values):
                continue

            # Extract forecast values for each ensemble member
            for ensemble_member in ensemble_members:
                forecast_data = ds['dis24'].sel(number=ensemble_member).isel(step=lead_time).values

                # Loop over the thresholds
                for severity, (obs_threshold, sim_threshold) in thresholds.items():
                    # Define events and non-events
                    observed_event = observed_values > obs_threshold
                    forecast_event = forecast_data > sim_threshold
                    
                    # Define tolerant events and non-events (within Â±2 days window)
                    observed_window = observed_data[(observed_data["date"] >= forecast_end_date - pd.DateOffset(days=2)) & 
                                                    (observed_data["date"] <= forecast_end_date + pd.DateOffset(days=2))]
                    tolerant_observed_event = (observed_window[station_name] > obs_threshold).any()
                    
                    # Create a dictionary to store results
                    events_dict = {
                        "forecast file": os.path.basename(forecast_file),
                        "lead time": lead_time,
                        "station name": station_name,
                        "ensemble member": ensemble_member,
                        "forecasted date": forecast_end_date.date(),
                        "threshold": severity,
                        "observed event": observed_event,
                        "forecast event": forecast_event,
                        "tolerant observed event": tolerant_observed_event
                    }
            
                    # Append the events dictionary to the list
                    events.append(events_dict)

# Create a data frame from the list of event dictionaries
events_df = pd.DataFrame(events)
print("Processing complete.")





# pivot the events_df to list ensemble members as columns
pivot_events_df = events_df.pivot_table(index=["forecast file", "lead time", "station name", "forecasted date","threshold", "observed event", "tolerant observed event"],
                                        columns="ensemble member",)


# reset index to convert the pivoted dataframe to a flat table structure
pivot_events_df.reset_index(inplace=True)


# define the columns corresponding to the forecast ensemble members (5 to 16)
ensemble_member_columns = pivot_events_df.columns[7:18]


# calculate the percentage of ensemble members with events (i.e., 1's) for each row
pivot_events_df["probability of detection"] = pivot_events_df[ensemble_member_columns].sum(axis=1) / len(ensemble_members)


# Generate trigger thresholds from 0.01 to 0.99 with a step size of 0.01
thresholds = np.arange(0.01, 1, 0.01)


# initialize an empty dataframe to store the event occurrence for each threshold
metrics_df = pd.DataFrame(index=pivot_events_df.index)





# loop over each threshold
for threshold in thresholds:
    # determine if the forecast event occurs based on the threshold
    event_occurrence = (pivot_events_df['probability of detection'] >= threshold).astype(int)
    
    # add the event occurrence as a column in the event_df DataFrame
    metrics_df[f'threshold_{threshold:.2f}'] = event_occurrence

# concatenate the forecast file and observed event columns from result_df to event_df
metrics_df = pd.concat([pivot_events_df[['forecast file','station name', 'lead time','forecasted date','threshold','observed event','tolerant observed event','probability of detection']], metrics_df], axis=1)

# flatten the multi-index columns
metrics_df.columns = [''.join(col).strip() for col in metrics_df.columns.values]





# define the lead time ranges (optional, change as desired)
bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 46, float('inf')]
labels = ['0-5', '6-10', '11-15', '16-20', '21-25', '26-30', '31-35', '36-40', '41-46', '0-46']

# create a new column called lead time category' that categorizes lead times into these ranges
metrics_df['lead time category'] = pd.cut(metrics_df['lead time'], bins=bins, labels=labels, right=False)

# group by station name, lead time category, and threshold
grouped = metrics_df.groupby(['station name', 'lead time category', 'threshold'], observed=False)

# create a dictionary to store each group's dataframe
grouped_dfs = {name: group for name, group in grouped}





# function to calculate verification metrics metrics for each grouped dataframe 
def calculate_metrics(df):
    hits = {}
    false_alarms = {}
    misses = {}
    correct_rejections = {}
    hit_rate = {}
    false_alarm_rate = {}
    csi = {}
    pss = {}
    f1_scores = {}
    

    # calculate contigency table metrics for each trigger threshold column
    for column in df.columns[8:107]:  # check columns from index 8 onwards are thresholds
        hits[column] = ((df['observed event'] == 1) & (df[column] == 1)).sum()
        false_alarms[column] = ((df['observed event'] == 0) & (df[column] == 1)).sum()
        misses[column] = ((df['observed event'] == 1) & (df[column] == 0)).sum()
        correct_rejections[column] = ((df['observed event'] == 0) & (df[column] == 0)).sum()
       
        # calculate verification metrics
        total_observed_events = hits[column] + misses[column]
        total_forecasted_events = hits[column] + false_alarms[column]
        hit_rate[column] = hits[column] / total_observed_events if total_observed_events > 0 else 0
        false_alarm_rate[column] = false_alarms[column] / total_forecasted_events if total_forecasted_events > 0 else 0
        csi[column] = hits[column] / (hits[column] + false_alarms[column] + misses[column]) if (hits[column] + false_alarms[column] + misses[column]) > 0 else 0
        pss[column] = (hits[column] * correct_rejections[column] - false_alarms[column] * misses[column]) / \
                      ((hits[column] + misses[column]) * (false_alarms[column] + correct_rejections[column])) if (hits[column] + misses[column]) * (false_alarms[column] + correct_rejections[column]) > 0 else 0
        precision = hits[column] / total_forecasted_events if total_forecasted_events > 0 else 0
        recall = hit_rate[column]
        f1_scores[column] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    # convert the metrics dictionaries to dataframes
    hits_df = pd.DataFrame(hits, index=['hits'])
    false_alarms_df = pd.DataFrame(false_alarms, index=['false alarms'])
    misses_df = pd.DataFrame(misses, index=['misses'])
    correct_rejections_df = pd.DataFrame(correct_rejections, index=['correct rejections'])
    hit_rate_df = pd.DataFrame(hit_rate, index=['hit rate'])
    false_alarm_rate_df = pd.DataFrame(false_alarm_rate, index=['false alarm rate'])
    
    csi_df = pd.DataFrame(csi, index=['csi'])
    pss_df = pd.DataFrame(pss, index=['pss'])
    f1_scores_df = pd.DataFrame(f1_scores, index=['f1 score'])
 
    # concatenate the metrics dataframes
    metrics_df = pd.concat([
        hits_df, false_alarms_df,
        misses_df, correct_rejections_df, hit_rate_df, false_alarm_rate_df,
        csi_df, pss_df, f1_scores_df,
    ])

    return metrics_df





# iterate through each dataframe in grouped_dfs, apply calculate_metrics, and store the results back into grouped_dfs
for key, df in grouped_dfs.items():
    grouped_dfs[key] = calculate_metrics(df)
    
    # timer end 
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"elapsed time for calculating metrics: {elapsed_time:.2f} seconds")





# view a specific grouped dataframe 
specific_df = grouped_dfs["Nyakapupu",'0-5','moderate']
specific_df





# Create an empty dictionary to store the best triggers
best_triggers = {}


# Iterate through each dataframe in grouped_dfs
for key, df in grouped_dfs.items():
    # Find the column names corresponding to thresholds
    threshold_columns = [col for col in df.columns if col.startswith('threshold_')]
    
    # Determine the maximum CSI value for each trigger
    max_csi = df.loc['csi'].max()
    
    # Find all trigger with the maximum CSI value
    best_csi_trigger = df.loc['csi'][df.loc['csi'] == max_csi].index.tolist()
    
    # If there are multiple triggers with the same CSI, choose the one with the highest hit rate
    if len(best_csi_trigger) > 1:
        # Find the hit rate for each of the best CSI triggers
        hit_rates = df.loc['hit rate'][best_csi_trigger]
        max_hit_rate = hit_rates.max()
        
        # Find all triggers with the maximum hit rate
        best_hit_rate_trigger = hit_rates[hit_rates == max_hit_rate].index.tolist()
        
        # If there are still ties, choose the trigger with the lowest value
        if len(best_hit_rate_trigger) > 1:
            # Convert trigger names to numbers (assuming trigger names follow a specific format)
            numeric_trigger = [float(thresh.split('_')[1]) for thresh in best_hit_rate_trigger]
            best_trigger = best_hit_rate_trigger[np.argmin(numeric_trigger)]  # Get the lowest threshold
        else:
            best_trigger = best_hit_rate_trigger[0]
    else:
        best_trigger = best_csi_trigger[0]
    
    # Store the best trigger information
    best_triggers[key] = {
        'best_trigger': best_trigger,
        'csi_value': df.loc['csi', best_trigger],
        'hit_rate': df.loc['hit rate', best_trigger]
    }

# Convert the best_triggers dictionary to a DataFrame for better readability
best_triggers_df = pd.DataFrame(best_triggers).T

# Print the DataFrame with the best triggers
print(best_triggers_df)



# Convert the best_triggers dictionary to a DataFrame for better readability
best_triggers_df = pd.DataFrame(best_triggers).T
best_triggers_df


# Save the results to a CSV file
best_triggers_df.to_csv(os.path.join(output_directory, f'triggers_{country}.csv'))







{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb284398-1f75-4754-9811-45b60b599b5b",
   "metadata": {},
   "source": [
    "This script processes the GloFAS data that you downloaded in the previous script. It takes the forecasts in GRIB format, reads in our metadata and extracts the values from the forecasts at the gauging stations of interest. We then output these values in csv format so they can be used for analysis such as comparisons to observed data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010fde2-e9e6-496d-9476-0043acec80a0",
   "metadata": {},
   "source": [
    "We first import the relevant packages from our environment file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777395ee-fa2b-4f1c-94b6-5d38a77cc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e379e1b9-6b5d-4721-9b2d-0534793d5e19",
   "metadata": {},
   "source": [
    "In the following cell we define our country of interest and home directory before setting up the paths to the relevant data. You should only have to change the directory variable here. An output directory to store the results will also be created. \n",
    "\n",
    "The metadata and forecast data will then be read in for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1a0c5-f05c-4e19-bd0a-b2182e19e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'zimbabwe'  # Define country of interest\n",
    "directory = '/Users/jamietowner/Documents/flood_aa_training/'  # Define directory\n",
    "\n",
    "# Define paths to data \n",
    "forecast_data_directory = os.path.join(directory, country, \"data/forecasts/glofas\")\n",
    "metadata_directory = os.path.join(directory, country, \"data/metadata\")\n",
    "\n",
    "# Load the CSV file containing station information (i.e., station name, lat, lon)\n",
    "station_info_file = \"metadata_zim.csv\"\n",
    "station_info_path = os.path.join(metadata_directory, station_info_file)\n",
    "station_info = pd.read_csv(station_info_path)\n",
    "\n",
    "# Define the output directory for CSV files\n",
    "output_directory = os.path.join(directory, country, \"data/forecasts/glofas/stations\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Get a list of all forecast GRIB files in the directory\n",
    "forecast_files = [f for f in os.listdir(forecast_data_directory) if f.endswith(\".grib\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9213d1-427f-4612-b651-d056ae4d97cb",
   "metadata": {},
   "source": [
    "To check our data has been read in correctly we can execute the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a46d3-e21f-470d-90eb-d8668323aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cf52fe-b0b9-4f4c-b124-3c1bebf1750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580487ec-56cf-4a9e-a227-383643d812d3",
   "metadata": {},
   "source": [
    "The rest of the script performs the hard work. It takes each forecast file in GRIB format and finds the relevant metadata from our station_info file (i.e., lat,lon,name) and uses this information to extract the river discharge values at the closest grid point to our station coordindates. Finally, each forecast is processed and the values are appended to a dataframe and are stored as a csv for each station and forecast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539e2c3-c5b5-4755-8b8b-4a6ee98a7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=len(forecast_files), desc=\"Extracting Data\")\n",
    "\n",
    "# Loop over each forecast file\n",
    "for forecast_file in forecast_files:\n",
    "    # Extract the date from the filename\n",
    "    forecast_date = forecast_file.split('_')[1].split('.')[0]  # Assuming GloFAS_YYYY_MM_DD.grib\n",
    "    \n",
    "    # Open the GRIB file\n",
    "    grib_file_path = os.path.join(forecast_data_directory, forecast_file)\n",
    "    ds = xr.open_dataset(grib_file_path, engine='cfgrib', filter_by_keys={'dataType': 'pf'})\n",
    "\n",
    "    # Find the names of latitude and longitude in the dataset\n",
    "    lat_name = 'latitude' if 'latitude' in ds.coords else 'lat'\n",
    "    lon_name = 'longitude' if 'longitude' in ds.coords else 'lon'\n",
    "\n",
    "    # Loop over each station in the metadata file\n",
    "    for index, row in station_info.iterrows():\n",
    "        point_name = row['station name']\n",
    "        latitude = row['lat']\n",
    "        longitude = row['lon']\n",
    "\n",
    "        # Find the nearest latitude and longitude in the dataset using the correct names\n",
    "        lat_index = ds[lat_name].sel(**{lat_name: latitude}, method='nearest')\n",
    "        lon_index = ds[lon_name].sel(**{lon_name: longitude}, method='nearest')\n",
    "\n",
    "        # Extract river discharge data for the nearest point and for all ensemble members\n",
    "        data_at_point = ds['dis24'].sel(**{lat_name: lat_index, lon_name: lon_index})\n",
    "\n",
    "        # Initialize a list to store data for all timesteps\n",
    "        all_steps_data = []\n",
    "\n",
    "        # Extract the date for each step\n",
    "        steps = ds.step.values\n",
    "        ensemble_count = len(ds['dis24'].coords['number'])\n",
    "\n",
    "        for step in steps:\n",
    "            # Extract data for this specific step\n",
    "            step_data = data_at_point.sel(step=step)\n",
    "\n",
    "            # Ensure step_data has the correct shape (should be [ensemble_count])\n",
    "            if step_data.shape[0] != ensemble_count:\n",
    "                print(f\"Shape mismatch for {point_name} on step {step}: expected {ensemble_count}, got {step_data.shape[0]}\")\n",
    "                continue  # Skip if there's a mismatch\n",
    "\n",
    "            # Reshape step_data for DataFrame creation\n",
    "            step_data_reshaped = step_data.values[:, np.newaxis]  # Reshape to (10, 1)\n",
    "\n",
    "            # Create a DataFrame for this step\n",
    "            df_step = pd.DataFrame(step_data_reshaped.T, columns=[f\"ensemble_{num}\" for num in ds['dis24'].coords['number'].values])\n",
    "\n",
    "            # Add the date for this step\n",
    "            date_value = pd.to_datetime(forecast_date) + pd.to_timedelta(step, unit='h')  # Assuming 24-hour timesteps\n",
    "            df_step['date'] = date_value\n",
    "\n",
    "            # Append to the list of all steps data\n",
    "            all_steps_data.append(df_step)\n",
    "\n",
    "        # Concatenate all step DataFrames into a single DataFrame\n",
    "        if all_steps_data:\n",
    "            final_df = pd.concat(all_steps_data, ignore_index=True)\n",
    "\n",
    "            # Rearrange columns to have date first\n",
    "            final_df = final_df[['date'] + [col for col in final_df.columns if col != 'date']]\n",
    "\n",
    "            # Format the date for the CSV file name\n",
    "            formatted_date = pd.to_datetime(forecast_date).strftime('%Y_%m_%d')\n",
    "\n",
    "            # Create the filename in the format: station_name_YYYY_MM_DD_glofas.csv\n",
    "            csv_file_name = os.path.join(output_directory, f\"{point_name}_{formatted_date}_glofas.csv\")\n",
    "\n",
    "            # Save the DataFrame to a CSV file\n",
    "            final_df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca38c2-eb24-4c79-8686-b6836ad5955f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
